{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['malo  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['minusv√°lido ?  '],\n",
       " ['ser  '],\n",
       " ['el  '],\n",
       " ['definitivamente  '],\n",
       " ['terrible  '],\n",
       " ['bebida  '],\n",
       " ['hotel  '],\n",
       " ['el  '],\n",
       " ['el  '],\n",
       " ['ser  '],\n",
       " ['muy  '],\n",
       " ['malo  '],\n",
       " ['tener  '],\n",
       " ['decepcionar  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['uno  '],\n",
       " ['muy  '],\n",
       " ['joyer√≠a  '],\n",
       " ['secuestro  '],\n",
       " ['para  '],\n",
       " ['¬ø qu√©  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['peor  '],\n",
       " ['sinverg√ºenza ! !  '],\n",
       " ['¬° no  '],\n",
       " ['boda  '],\n",
       " ['malo  '],\n",
       " ['timo  '],\n",
       " ['no  '],\n",
       " ['servicio  '],\n",
       " ['servicio  '],\n",
       " ['vacaciones  '],\n",
       " ['genial ,  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['evitar  '],\n",
       " ['desayuno  '],\n",
       " ['fatal  '],\n",
       " ['horrible  '],\n",
       " ['timo  '],\n",
       " ['bonito  '],\n",
       " ['nunca  '],\n",
       " ['no  '],\n",
       " ['caro  '],\n",
       " ['decepcionante  '],\n",
       " ['llegar  '],\n",
       " ['de el  '],\n",
       " ['decepcionar  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['haber  '],\n",
       " ['enga√±o  '],\n",
       " ['ni  '],\n",
       " ['malo  '],\n",
       " ['peor  '],\n",
       " ['muy  '],\n",
       " ['malo  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " ['cucaracha  '],\n",
       " ['malo  '],\n",
       " ['malo . '],\n",
       " ['cuidado  '],\n",
       " ['nunca  '],\n",
       " ['no  '],\n",
       " ['peor  '],\n",
       " ['el  '],\n",
       " ['decepcionante ,  '],\n",
       " ['experiencia  '],\n",
       " ['hotel  '],\n",
       " ['mediocre  '],\n",
       " ['comida  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['decepci√≥n  '],\n",
       " ['cucaracha  '],\n",
       " ['el  '],\n",
       " ['no  '],\n",
       " ['vacaciones  '],\n",
       " ['ni  '],\n",
       " ['el  '],\n",
       " ['pesimo  '],\n",
       " ['muy  '],\n",
       " ['terrible  '],\n",
       " ['el  '],\n",
       " ['hu√©sped  '],\n",
       " ['servicio  '],\n",
       " ['outrageouly  '],\n",
       " ['excelente  '],\n",
       " ['espantoso  '],\n",
       " ['cerrar  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['uno  '],\n",
       " ['muy  '],\n",
       " ['para  '],\n",
       " ['1  '],\n",
       " ['mal  '],\n",
       " ['si  '],\n",
       " ['ni  '],\n",
       " ['tambi√©n  '],\n",
       " ['totalmente  '],\n",
       " ['grupal  '],\n",
       " ['pesima  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['igual  '],\n",
       " ['decepcionar  '],\n",
       " ['muy  '],\n",
       " ['tratamiento  '],\n",
       " ['precioso ,  '],\n",
       " ['sin  '],\n",
       " ['üò†  '],\n",
       " ['horrible  '],\n",
       " ['evitar  '],\n",
       " ['atenci√≥n  '],\n",
       " ['uno  '],\n",
       " ['decepcionar  '],\n",
       " ['mi  '],\n",
       " ['inicio  '],\n",
       " ['haber  '],\n",
       " ['el  '],\n",
       " ['malo  '],\n",
       " ['experiencia  '],\n",
       " ['el  '],\n",
       " ['multipropiedad  '],\n",
       " ['malo  '],\n",
       " ['camida  '],\n",
       " ['dolor  '],\n",
       " ['terrible  '],\n",
       " ['nada  '],\n",
       " ['alberca  '],\n",
       " ['alejar  '],\n",
       " ['el  '],\n",
       " ['no  '],\n",
       " ['booourns  '],\n",
       " ['pesimo . '],\n",
       " ['gran  '],\n",
       " ['enga√±oso  '],\n",
       " ['malo  '],\n",
       " ['fraude  '],\n",
       " ['uno  '],\n",
       " ['actitud  '],\n",
       " ['no  '],\n",
       " ['5  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['falta  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['horrible  '],\n",
       " ['cena  '],\n",
       " ['estar  '],\n",
       " ['terrible  '],\n",
       " ['falta  '],\n",
       " ['. '],\n",
       " ['gran  '],\n",
       " ['estar  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['precioso  '],\n",
       " ['caro  '],\n",
       " ['tener  '],\n",
       " ['grandluxxe  '],\n",
       " ['sacar  '],\n",
       " ['sin  '],\n",
       " ['restaurante  '],\n",
       " ['sin  '],\n",
       " ['gran  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['hotel  '],\n",
       " [''],\n",
       " ['ten_miel  '],\n",
       " ['peor  '],\n",
       " ['no  '],\n",
       " ['peor  '],\n",
       " ['horrible  '],\n",
       " ['el  '],\n",
       " ['el  '],\n",
       " ['interesante  '],\n",
       " ['el  '],\n",
       " ['el  '],\n",
       " ['sin  '],\n",
       " ['fyi  '],\n",
       " ['vacaciones  '],\n",
       " ['malo  '],\n",
       " ['peor  '],\n",
       " ['malo  '],\n",
       " ['preparir  '],\n",
       " ['estar  '],\n",
       " ['muy  '],\n",
       " ['no  '],\n",
       " ['servicio  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['el  '],\n",
       " ['uno  '],\n",
       " ['2016-12-01  '],\n",
       " ['ser  '],\n",
       " ['ser  '],\n",
       " ['malo  '],\n",
       " ['¬° alerta ,  '],\n",
       " ['gran  '],\n",
       " ['el  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['peor  '],\n",
       " ['moho  '],\n",
       " ['no  '],\n",
       " ['sorprender  '],\n",
       " ['ex-agente  '],\n",
       " ['malo  '],\n",
       " ['evitar  '],\n",
       " ['malo  '],\n",
       " ['muy  '],\n",
       " ['cena  '],\n",
       " ['malo  '],\n",
       " ['lamentable  '],\n",
       " ['vacaciones  '],\n",
       " ['si  '],\n",
       " ['¬° uno  '],\n",
       " ['gran  '],\n",
       " [''],\n",
       " [''],\n",
       " ['ten a inoportuno  '],\n",
       " ['estafa  '],\n",
       " ['cada  '],\n",
       " ['muy  '],\n",
       " ['conseguir  '],\n",
       " ['el  '],\n",
       " ['regresar  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['mal  '],\n",
       " ['servicio  '],\n",
       " ['total  '],\n",
       " ['caro  '],\n",
       " ['decepcionante  '],\n",
       " ['no  '],\n",
       " ['lleno  '],\n",
       " ['malo  '],\n",
       " ['malisimo  '],\n",
       " ['excesivamente  '],\n",
       " ['el  '],\n",
       " ['malo  '],\n",
       " ['3  '],\n",
       " ['servicio  '],\n",
       " ['nadar  '],\n",
       " ['malo  '],\n",
       " ['cerradura  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " [''],\n",
       " ['ten pesimo  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['mugre  '],\n",
       " ['mal  '],\n",
       " ['6  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['me  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['terrible  '],\n",
       " ['malo  '],\n",
       " ['evitar  '],\n",
       " ['el  '],\n",
       " ['espantoso  '],\n",
       " ['muy  '],\n",
       " ['ni  '],\n",
       " ['necesidad  '],\n",
       " ['the  '],\n",
       " ['nunca  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['vacaciones  '],\n",
       " ['malo  '],\n",
       " ['el  '],\n",
       " ['alejar  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['comida  '],\n",
       " ['de  '],\n",
       " ['gran  '],\n",
       " ['no  '],\n",
       " ['salir  '],\n",
       " ['cuidado  '],\n",
       " ['no  '],\n",
       " ['moho  '],\n",
       " ['comenzar  '],\n",
       " ['mal  '],\n",
       " ['prepotente  '],\n",
       " ['excelente  '],\n",
       " ['pesimo  '],\n",
       " ['el  '],\n",
       " ['no  '],\n",
       " ['horror  '],\n",
       " ['trampa  '],\n",
       " ['extremadamente  '],\n",
       " ['mal  '],\n",
       " ['bad  '],\n",
       " ['fallo  '],\n",
       " ['no  '],\n",
       " ['probar  '],\n",
       " ['evitar  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " ['sin  '],\n",
       " ['baja  '],\n",
       " ['alojar  '],\n",
       " ['decepci√≥n  '],\n",
       " ['cucaracha ! ! !  '],\n",
       " ['pesima  '],\n",
       " ['grand  '],\n",
       " ['excelente  '],\n",
       " ['lastimar  '],\n",
       " ['tratamiento  '],\n",
       " ['malo  '],\n",
       " ['hotel  '],\n",
       " ['no  '],\n",
       " ['dejar  '],\n",
       " ['el  '],\n",
       " ['poder  '],\n",
       " ['mal  '],\n",
       " ['mucho  '],\n",
       " ['el  '],\n",
       " ['el  '],\n",
       " ['apestar  '],\n",
       " ['por  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['puaj  '],\n",
       " ['sonoro  '],\n",
       " ['terrible  '],\n",
       " ['el  '],\n",
       " ['todo  '],\n",
       " ['vacaciones  '],\n",
       " ['genial  '],\n",
       " ['robooo  '],\n",
       " ['compartir  '],\n",
       " ['muy  '],\n",
       " ['pesimo  '],\n",
       " ['grosero ,  '],\n",
       " ['muy  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " [''],\n",
       " ['$ $ $ terribles  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['enfermo  '],\n",
       " ['peligro ,  '],\n",
       " ['muy  '],\n",
       " ['lujoso ,  '],\n",
       " ['mal  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['todav√≠a  '],\n",
       " ['desayuno  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['peor  '],\n",
       " ['uno  '],\n",
       " ['nunca  '],\n",
       " ['malo  '],\n",
       " ['el  '],\n",
       " ['cena  '],\n",
       " ['malo  '],\n",
       " ['mal  '],\n",
       " ['ruido  '],\n",
       " ['pesimo  '],\n",
       " ['malo  '],\n",
       " ['peor  '],\n",
       " ['desagradable  '],\n",
       " ['tanto  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['sobrevalorar  '],\n",
       " ['malo  '],\n",
       " ['el  '],\n",
       " ['asco  '],\n",
       " ['el  '],\n",
       " ['uno  '],\n",
       " ['mal  '],\n",
       " ['malo  '],\n",
       " ['necesitar  '],\n",
       " ['no  '],\n",
       " ['haber  '],\n",
       " ['tiempo  '],\n",
       " ['excesivo  '],\n",
       " ['ir  '],\n",
       " ['presentaci√≥n  '],\n",
       " ['nunca  '],\n",
       " ['bueno  '],\n",
       " ['malo  '],\n",
       " ['dejo  '],\n",
       " ['piaf  '],\n",
       " ['evitar  '],\n",
       " ['blah !  '],\n",
       " ['grande  '],\n",
       " ['muy  '],\n",
       " ['malo  '],\n",
       " ['si  '],\n",
       " ['inmensamente  '],\n",
       " ['decepci√≥n  '],\n",
       " ['discriminaci√≥n  '],\n",
       " ['ojo ! ! !  '],\n",
       " ['decepcionar  '],\n",
       " ['malo  '],\n",
       " [''],\n",
       " ['ten_adem√°s  '],\n",
       " ['robar  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['lamentablemente  '],\n",
       " ['malo  '],\n",
       " ['mediocridad  '],\n",
       " ['actitud  '],\n",
       " ['el  '],\n",
       " ['escaso  '],\n",
       " [''],\n",
       " ['ten_decepcionante  '],\n",
       " ['muy  '],\n",
       " ['uno  '],\n",
       " ['pesimo  '],\n",
       " ['malo  '],\n",
       " ['servicio  '],\n",
       " ['abuso  '],\n",
       " ['el  '],\n",
       " ['trampa  '],\n",
       " ['terrible  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " ['comprobar  '],\n",
       " ['uno  '],\n",
       " ['uno  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['desayuno  '],\n",
       " ['caro !  '],\n",
       " ['malo  '],\n",
       " ['malo  '],\n",
       " ['ser  '],\n",
       " ['pesima  '],\n",
       " ['uno  '],\n",
       " ['estancia  '],\n",
       " ['¬° qu√©  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['evitar  '],\n",
       " ['mal . '],\n",
       " ['hotel  '],\n",
       " ['¬° momia ,  '],\n",
       " ['comida  '],\n",
       " ['no  '],\n",
       " ['momificar  '],\n",
       " ['inc√≥modo  '],\n",
       " ['muy  '],\n",
       " ['horrible  '],\n",
       " ['odioso  '],\n",
       " ['no  '],\n",
       " ['esperar  '],\n",
       " ['no  '],\n",
       " ['ir  '],\n",
       " ['gran  '],\n",
       " ['no  '],\n",
       " ['malo  '],\n",
       " ['muy  '],\n",
       " ['lugar  '],\n",
       " ['no  '],\n",
       " ['buff  '],\n",
       " ['malo  '],\n",
       " ['0  '],\n",
       " ['deber  '],\n",
       " ['tomar  '],\n",
       " ['nada  '],\n",
       " ['peque√±o  '],\n",
       " ['mal  '],\n",
       " ['puro  '],\n",
       " ['nada  '],\n",
       " ['uno  '],\n",
       " ['tener  '],\n",
       " ['¬° que  '],\n",
       " ['horrible  '],\n",
       " ['uno  '],\n",
       " ['momia  '],\n",
       " ['horrible  '],\n",
       " ['¬° uno  '],\n",
       " ['¬° t√©trico !  '],\n",
       " ['nada  '],\n",
       " ['de  '],\n",
       " ['¬° malo !  '],\n",
       " ['ocupar  '],\n",
       " ['no  '],\n",
       " ['es  '],\n",
       " ['uno  '],\n",
       " ['horrible  '],\n",
       " ['decepcionante  '],\n",
       " ['oler  '],\n",
       " ['no  '],\n",
       " ['morbo  '],\n",
       " ['me  '],\n",
       " ['totalmente  '],\n",
       " ['aburr√≠disimo  '],\n",
       " ['no  '],\n",
       " ['sufrir  '],\n",
       " [\"don't  \"],\n",
       " ['muy  '],\n",
       " ['no  '],\n",
       " ['abandonar  '],\n",
       " ['emblem√°tico  '],\n",
       " ['sucio  '],\n",
       " ['decepci√≥n  '],\n",
       " ['decepcionante  '],\n",
       " ['¬° atascado !  '],\n",
       " ['el  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " ['peque√±o ,  '],\n",
       " ['olor  '],\n",
       " ['excelente  '],\n",
       " ['descuidado  '],\n",
       " ['¬° malo !  '],\n",
       " ['callej√≥n  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " ['horrible  '],\n",
       " ['ser  '],\n",
       " ['de  '],\n",
       " ['no  '],\n",
       " ['cual  '],\n",
       " ['visita  '],\n",
       " ['grand  '],\n",
       " ['ese  '],\n",
       " ['defraudar  '],\n",
       " ['maravilloso  '],\n",
       " ['normalmente  '],\n",
       " ['ni  '],\n",
       " ['principalmente  '],\n",
       " ['servicio  '],\n",
       " ['mal  '],\n",
       " ['normal-mediocre  '],\n",
       " ['grand  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['pesima  '],\n",
       " ['rebasar  '],\n",
       " ['medio  '],\n",
       " ['1  '],\n",
       " ['¬ø c√≥mo  '],\n",
       " ['demasiado  '],\n",
       " ['robo  '],\n",
       " ['haber  '],\n",
       " ['pesimo  '],\n",
       " ['decepcionar  '],\n",
       " ['no  '],\n",
       " ['mal  '],\n",
       " ['quedar  '],\n",
       " ['el  '],\n",
       " ['construcci√≥n  '],\n",
       " ['muy  '],\n",
       " ['problema  '],\n",
       " ['sobrevalorar  '],\n",
       " ['no  '],\n",
       " ['cada  '],\n",
       " ['no  '],\n",
       " ['decepcionante  '],\n",
       " ['horrible  '],\n",
       " ['lista  '],\n",
       " ['obtener  '],\n",
       " ['uno  '],\n",
       " ['oler  '],\n",
       " ['moho  '],\n",
       " ['el  '],\n",
       " ['nunca  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['entorno  '],\n",
       " ['si  '],\n",
       " ['hermoso  '],\n",
       " ['estar  '],\n",
       " ['no  '],\n",
       " ['atrapar  '],\n",
       " ['muy  '],\n",
       " ['caro ,  '],\n",
       " ['no  '],\n",
       " ['solo  '],\n",
       " ['estafa . '],\n",
       " ['mal  '],\n",
       " ['atrapar  '],\n",
       " ['√©xito  '],\n",
       " ['hotel  '],\n",
       " ['decepci√≥n  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['ni  '],\n",
       " ['propietario  '],\n",
       " ['muy ,  '],\n",
       " ['este  '],\n",
       " ['no  '],\n",
       " ['pro  '],\n",
       " ['para  '],\n",
       " ['villa  '],\n",
       " ['friamente  '],\n",
       " ['ser  '],\n",
       " ['alguno  '],\n",
       " ['bueno  '],\n",
       " ['mejora  '],\n",
       " ['muy  '],\n",
       " ['en  '],\n",
       " ['desilucionado  '],\n",
       " ['comida  '],\n",
       " ['italiano ?  '],\n",
       " ['mucho  '],\n",
       " ['servicio  '],\n",
       " ['platillo  '],\n",
       " ['mal  '],\n",
       " ['caro ,  '],\n",
       " ['probar  '],\n",
       " ['ok  '],\n",
       " ['sobrevaluar  '],\n",
       " ['ir  '],\n",
       " ['bernice ,  '],\n",
       " ['decepcionar  '],\n",
       " ['1  '],\n",
       " ['desayuno  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['l√°stima  '],\n",
       " ['2  '],\n",
       " ['tomar  '],\n",
       " ['¬ø cu√°l  '],\n",
       " ['uno  '],\n",
       " ['ser  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['mal  '],\n",
       " ['de  '],\n",
       " ['hoy  '],\n",
       " ['2  '],\n",
       " ['no  '],\n",
       " ['decepcionante  '],\n",
       " ['decepcionante  '],\n",
       " ['el  '],\n",
       " ['menos  '],\n",
       " ['todo  '],\n",
       " ['ser  '],\n",
       " ['no  '],\n",
       " ['uno  '],\n",
       " ['malo  '],\n",
       " ['trampa  '],\n",
       " ['sucio  '],\n",
       " ['tan  '],\n",
       " ['uno  '],\n",
       " ['porci√≥n  '],\n",
       " ['personal  '],\n",
       " ['muy  '],\n",
       " ['hotel  '],\n",
       " ['esperar  '],\n",
       " ['no  '],\n",
       " ['decepcionar  '],\n",
       " ['falta  '],\n",
       " ['por  '],\n",
       " ['no  '],\n",
       " ['antes  '],\n",
       " ['mal  '],\n",
       " [''],\n",
       " ['ten_mala  '],\n",
       " ['mas  '],\n",
       " ['bonito  '],\n",
       " ['no  '],\n",
       " ['mando  '],\n",
       " ['gran  '],\n",
       " ['not  '],\n",
       " ['hacer  '],\n",
       " ['el  '],\n",
       " ['sobrevalorar  '],\n",
       " ['hard  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['ya  '],\n",
       " ['no  '],\n",
       " ['peor  '],\n",
       " ['nunca  '],\n",
       " ['malo  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['bullicio  '],\n",
       " ['bueno  '],\n",
       " ['muy  '],\n",
       " ['el  '],\n",
       " ['muy  '],\n",
       " ['ingrediente  '],\n",
       " ['est√∫pido  '],\n",
       " ['cobrar  '],\n",
       " ['decepcionar  '],\n",
       " ['no  '],\n",
       " ['breve  '],\n",
       " ['torre  '],\n",
       " [''],\n",
       " ['ten_despu√©s  '],\n",
       " ['ejecuci√≥n  '],\n",
       " ['simplemente  '],\n",
       " ['bar  '],\n",
       " ['el  '],\n",
       " ['gente  '],\n",
       " ['precio  '],\n",
       " ['mantener  '],\n",
       " ['habitaci√≥n  '],\n",
       " ['dudoso  '],\n",
       " ['hermoso  '],\n",
       " ['triste  '],\n",
       " ['ruidoso ,  '],\n",
       " ['ser  '],\n",
       " ['2  '],\n",
       " ['decepcionar  '],\n",
       " ['absolutamente  '],\n",
       " ['mucho  '],\n",
       " ['bueno  '],\n",
       " ['moda  '],\n",
       " ['no  '],\n",
       " ['pobre  '],\n",
       " ['s√≥lo  '],\n",
       " ['personal  '],\n",
       " ['genial  '],\n",
       " ['estado  '],\n",
       " ['no  '],\n",
       " ['poder  '],\n",
       " ['precioso  '],\n",
       " ['no  '],\n",
       " ['agradable  '],\n",
       " ['el  '],\n",
       " ['clasificaci√≥n  '],\n",
       " ['uno  '],\n",
       " ['no  '],\n",
       " ['viejo ,  '],\n",
       " ['no  '],\n",
       " ['este  '],\n",
       " ['no  '],\n",
       " ['enormemente  '],\n",
       " ['muy  '],\n",
       " ['continuar  '],\n",
       " ['sucio  '],\n",
       " ['instalaci√≥n  '],\n",
       " ['regular  '],\n",
       " ['decepcionante . '],\n",
       " ['malo  '],\n",
       " ['decepcionante . '],\n",
       " ['demasiado  '],\n",
       " ['renovar  '],\n",
       " ['muy  '],\n",
       " ['m√°s  '],\n",
       " ['vacaciones  '],\n",
       " ['mediocre  '],\n",
       " ['terrible  '],\n",
       " ['genial  '],\n",
       " ['mal  '],\n",
       " ['seguridad  '],\n",
       " ['bonito ,  '],\n",
       " ['mentalidad  '],\n",
       " ['servicio  '],\n",
       " ['mas  '],\n",
       " ['pesimo  '],\n",
       " ['nuestro  '],\n",
       " ['caro  '],\n",
       " ['nada  '],\n",
       " ['experiencia  '],\n",
       " ['el  '],\n",
       " ['vacaciones  '],\n",
       " ['parte  '],\n",
       " ['prisi√≥n  '],\n",
       " ['servicio  '],\n",
       " ['aviso  '],\n",
       " ['hard  '],\n",
       " ['el  '],\n",
       " ['el  '],\n",
       " ['mal  '],\n",
       " ['uno  '],\n",
       " ['servicio  '],\n",
       " ['buffet  '],\n",
       " ['el  '],\n",
       " ['uno  '],\n",
       " ['comida  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['si  '],\n",
       " ['irregular  '],\n",
       " ['malo  '],\n",
       " ['el  '],\n",
       " ['no  '],\n",
       " ['precio  '],\n",
       " ['de  '],\n",
       " ['no  '],\n",
       " ['para  '],\n",
       " ['2  '],\n",
       " ['no  '],\n",
       " ['ser  '],\n",
       " ['el  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['fiesta  '],\n",
       " ['no  '],\n",
       " ['comida  '],\n",
       " [''],\n",
       " ['$14 proceso  '],\n",
       " ['imitaci√≥n  '],\n",
       " ['|  '],\n",
       " ['bueno  '],\n",
       " ['nunca  '],\n",
       " ['no  '],\n",
       " ['demasiado  '],\n",
       " ['flan  '],\n",
       " ['no  '],\n",
       " ['mano  '],\n",
       " ['bueno  '],\n",
       " ['nunca  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['muy  '],\n",
       " ['noche  '],\n",
       " ['ser  '],\n",
       " ['si  '],\n",
       " ['el  '],\n",
       " ['el  '],\n",
       " ['preferencia  '],\n",
       " ['celebraci√≥n  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['disappoiontment  '],\n",
       " ['hotel  '],\n",
       " ['no  '],\n",
       " ['parada !  '],\n",
       " ['malo  '],\n",
       " ['muy  '],\n",
       " ['no  '],\n",
       " ['gran  '],\n",
       " ['decepcionar  '],\n",
       " ['aislar  '],\n",
       " ['soler  '],\n",
       " ['no  '],\n",
       " ['el  '],\n",
       " ['¬ø qu√©  '],\n",
       " ['mantener  '],\n",
       " ['gran  '],\n",
       " [''],\n",
       " ['ten_leeeento  '],\n",
       " ['lento  '],\n",
       " ['me  '],\n",
       " ['ojal√°  '],\n",
       " ['todo  '],\n",
       " ['muy  '],\n",
       " ['no  '],\n",
       " ['uno  '],\n",
       " ['perdon  '],\n",
       " [''],\n",
       " ['x uno  '],\n",
       " ['queja  '],\n",
       " ['el  '],\n",
       " ['ubicaci√≥n  '],\n",
       " ['decepcionante  '],\n",
       " ['exceso  '],\n",
       " ['intoxicaci√≥n  '],\n",
       " ['el  '],\n",
       " ['mal  '],\n",
       " ['que  '],\n",
       " ['el  '],\n",
       " ['elvis  '],\n",
       " ['haber  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['ser  '],\n",
       " ['hot  '],\n",
       " ['no  '],\n",
       " ['decepci√≥n  '],\n",
       " ['2-1  '],\n",
       " ['que  '],\n",
       " ['parecer  '],\n",
       " ['no  '],\n",
       " ['uno  '],\n",
       " ['uno  '],\n",
       " ['medio  '],\n",
       " ['grand  '],\n",
       " ['villa  '],\n",
       " ['pesima  '],\n",
       " ['evitar  '],\n",
       " ['no  '],\n",
       " ['estar  '],\n",
       " ['no  '],\n",
       " ['bajo  '],\n",
       " ['estar  '],\n",
       " ['comida  '],\n",
       " ['decepcionar  '],\n",
       " ['malo  '],\n",
       " ['sobrevalorar  '],\n",
       " ['el  '],\n",
       " ['ir  '],\n",
       " ['esperar  '],\n",
       " ['mal  '],\n",
       " ['terrible  '],\n",
       " ['5  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['ya  '],\n",
       " [''],\n",
       " ['ten_mal  '],\n",
       " ['menos  '],\n",
       " ['comida  '],\n",
       " ['grand  '],\n",
       " ['muy  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['mal  '],\n",
       " ['muy  '],\n",
       " ['decepcionante  '],\n",
       " ['desayuno  '],\n",
       " [''],\n",
       " ['a_se  '],\n",
       " ['desaparecer  '],\n",
       " ['el  '],\n",
       " ['hotel  '],\n",
       " ['intervalo  '],\n",
       " ['mal  '],\n",
       " ['ambiente  '],\n",
       " ['el  '],\n",
       " ['caro ,  '],\n",
       " ['cara  '],\n",
       " ['mucho  '],\n",
       " ['pizza  '],\n",
       " ['ya  '],\n",
       " ['bueno  '],\n",
       " ['malo  '],\n",
       " ['uno  '],\n",
       " ['vacaciones  '],\n",
       " ['no  '],\n",
       " ['muy  '],\n",
       " ['sobrevalorar  '],\n",
       " ['aceptable  '],\n",
       " ['horrible  '],\n",
       " ['servicio  '],\n",
       " ['decepcionar  '],\n",
       " ['decepcionar  '],\n",
       " ['no  '],\n",
       " ['mal  '],\n",
       " ['evitar  '],\n",
       " ['experiencia  '],\n",
       " ['esperar  '],\n",
       " ['no  '],\n",
       " ['mediocre  '],\n",
       " ['en  '],\n",
       " ['pudrir  '],\n",
       " ['decepcionante . '],\n",
       " ['como  '],\n",
       " ['que  '],\n",
       " ['muy  '],\n",
       " ['comida  '],\n",
       " [''],\n",
       " ['$14 mucho  '],\n",
       " ['bien  '],\n",
       " ['caro  '],\n",
       " ['probablemente  '],\n",
       " ['hermoso  '],\n",
       " ['cena  '],\n",
       " ['bien  '],\n",
       " ['muy  '],\n",
       " ['decepcionante . '],\n",
       " ['decepci√≥n  '],\n",
       " ['bueno ,  '],\n",
       " ['descriminar  '],\n",
       " ['demasiado  '],\n",
       " ['ojo  '],\n",
       " ['nos  '],\n",
       " ['sugerencia  '],\n",
       " ['muy  '],\n",
       " ['agradable  '],\n",
       " ['comida  '],\n",
       " ['alojar  '],\n",
       " ['el  '],\n",
       " ['lastimar  '],\n",
       " ['no  '],\n",
       " ['no  '],\n",
       " ['rep√∫blica  '],\n",
       " ['. '],\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not (os.path.exists('corpus_lematizado.pkl')):\n",
    "\tprint ('no se ha generado el corpus lematizado')\n",
    "else:\n",
    "\tcorpus_file = open ('corpus_lematizado.pkl','rb')\n",
    "\tcorpus_lematizado = pickle.load(corpus_file)\n",
    "\n",
    "corpus_lematizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000002untitled?line=1'>2</a>\u001b[0m vectorizador_binario \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000002untitled?line=2'>3</a>\u001b[0m \u001b[39m#vectorizador_binario = CountVectorizer(binary=True, stop_words=list_stop_words)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000002untitled?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizador_binario\u001b[39m.\u001b[39;49mfit_transform(texts)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000002untitled?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m (vectorizador_binario\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000002untitled?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m (X)\u001b[39m#sparse matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=110'>111</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=111'>112</a>\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=113'>114</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=114'>115</a>\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=51'>52</a>\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=52'>53</a>\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=53'>54</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=67'>68</a>\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=68'>69</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=70'>71</a>\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=72'>73</a>\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Representaci√≥n vectorial binarizada\n",
    "vectorizador_binario = CountVectorizer(binary=True)\n",
    "#vectorizador_binario = CountVectorizer(binary=True, stop_words=list_stop_words)\n",
    "X = vectorizador_binario.fit_transform(texts)\n",
    "print (vectorizador_binario.get_feature_names_out())\n",
    "print (X)#sparse matrix\n",
    "print (type(X.toarray()))#dense ndarray\n",
    "print ('Representaci√≥n vectorial binarizada')\n",
    "print (X.toarray())#dense ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000003untitled?line=0'>1</a>\u001b[0m \u001b[39m#Representaci√≥n vectorial por frecuencia\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000003untitled?line=1'>2</a>\u001b[0m vectorizador_frecuencia \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000003untitled?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizador_frecuencia\u001b[39m.\u001b[39;49mfit_transform(corpus_lematizado)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000003untitled?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRepresentaci√≥n vectorial por frecuencia\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000003untitled?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m (X\u001b[39m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=110'>111</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=111'>112</a>\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=113'>114</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=114'>115</a>\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=51'>52</a>\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=52'>53</a>\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=53'>54</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=67'>68</a>\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=68'>69</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=70'>71</a>\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=72'>73</a>\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#Representaci√≥n vectorial por frecuencia\n",
    "vectorizador_frecuencia = CountVectorizer()\n",
    "X = vectorizador_frecuencia.fit_transform(corpus_lematizado)\n",
    "print('Representaci√≥n vectorial por frecuencia')\n",
    "print (X.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000004untitled?line=0'>1</a>\u001b[0m \u001b[39m#~ #Representaci√≥n vectorial tf-idf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000004untitled?line=1'>2</a>\u001b[0m vectorizador_tfidf \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000004untitled?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizador_tfidf\u001b[39m.\u001b[39;49mfit_transform(corpus_lematizado)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000004untitled?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mRepresentaci√≥n vectorial tf-idf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000004untitled?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m (X\u001b[39m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2057'>2058</a>\u001b[0m \u001b[39m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2058'>2059</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2059'>2060</a>\u001b[0m \u001b[39mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2073'>2074</a>\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2074'>2075</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2075'>2076</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2076'>2077</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2077'>2078</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2078'>2079</a>\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=2079'>2080</a>\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=110'>111</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=111'>112</a>\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=113'>114</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=114'>115</a>\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=51'>52</a>\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=52'>53</a>\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=53'>54</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=67'>68</a>\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=68'>69</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=70'>71</a>\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/vaps/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py?line=72'>73</a>\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#~ #Representaci√≥n vectorial tf-idf\n",
    "vectorizador_tfidf = TfidfVectorizer()\n",
    "X = vectorizador_tfidf.fit_transform(corpus_lematizado)\n",
    "print ('Representaci√≥n vectorial tf-idf')\n",
    "print (X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
